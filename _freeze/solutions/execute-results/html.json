{
  "hash": "1d3c8dcdadcdc838ac88d9a7f02e301b",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Chapter solutions\"\nauthor: \"Barry Quinn\"\neditor: visual\n---\n\n\n\n\n\n\n## Chapter 1: Statistics and Probability Primer\n\n1.  Calculating Stock Returns:\n    **Objective:** To calculate the annualized return of a stock over a three-year period.\n    **Solution:**\n\n\n\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    # Initial and final prices of the stock\n    initial_price <- 100\n    final_price <- 150\n    \n    # Investment period in years\n    years <- 3\n    \n    # Calculating the annualized return\n    annualized_return <- (final_price / initial_price)^(1/years) - 1\n    \n    # Output the annualized return\n    print(annualized_return)\n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    \n    ```\n    [1] 0.1447142\n    ```\n    \n    \n    :::\n    :::\n\n\n\n\n\n    **Explanation:** The annualized return is calculated by finding the geometric average of the yearly return. It accounts for compounding over the period.\n\n2.  Descriptive Statistics of Financial Data:\n    **Objective:** To summarize and interpret a dataset of stock prices.\n    **Solution:**\n\n\n\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    # Dataset of stock prices\n    stock_prices <- c(120, 125, 130, 128, 135)\n    \n    # Summary statistics\n    summary_stats <- summary(stock_prices)\n    \n    # Output the summary statistics\n    print(summary_stats)\n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    \n    ```\n       Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n      120.0   125.0   128.0   127.6   130.0   135.0 \n    ```\n    \n    \n    :::\n    :::\n\n\n\n\n\n    **Explanation:** `summary()` function in R provides a quick statistical summary of the data, including measures like minimum, first quartile, median, mean, third quartile, and maximum.\n\n3.  Basic Risk Assessment:\n    **Objective:** To calculate and interpret the standard deviation of stock returns as a measure of risk.\n    **Solution:**\n\n\n\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    # Returns of a stock\n    stock_returns <- c(0.05, 0.02, -0.03, 0.04, 0.01)\n    \n    # Calculating standard deviation\n    risk_measure <- sd(stock_returns)\n    \n    # Output the standard deviation\n    print(risk_measure)\n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    \n    ```\n    [1] 0.03114482\n    ```\n    \n    \n    :::\n    :::\n\n\n\n\n\n    **Explanation:** The standard deviation provides a measure of the dispersion of returns. A higher standard deviation implies greater risk (volatility) in the stock's returns.\n\n4.  Simple Probability Calculation:\n    **Objective:** To calculate the probability of an event in a financial context, exemplified by a coin toss.\n    **Solution:**\n\n\n\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    # Probability of getting heads in a fair coin toss\n    probability_heads <- 1 / 2\n    \n    # Output the probability\n    print(probability_heads)\n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    \n    ```\n    [1] 0.5\n    ```\n    \n    \n    :::\n    :::\n\n\n\n\n\n    **Explanation:** This is a basic example of classical probability, where each outcome (heads or tails) is equally likely.\n\n5.  Basic Time Series Forecasting:\n    **Objective:** To use a simple moving average for forecasting the next data point in a financial time series.\n    **Solution:**\n\n\n\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    # Historical stock prices\n    stock_prices <- c(120, 122, 121, 123, 125)\n    \n    # Forecast using a simple moving average of the last 3 prices\n    forecast_price <- mean(tail(stock_prices, n=3))\n    \n    # Output the forecasted price\n    print(forecast_price)\n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    \n    ```\n    [1] 123\n    ```\n    \n    \n    :::\n    :::\n\n\n\n\n\n    **Explanation:** This method forecasts the next data point by calculating the average of a specified number of the most recent data points (here, the last three prices).\n\n6.  Advanced Risk Modeling (VaR):\n    **Objective:** To calculate and interpret the Value at Risk (VaR) for a portfolio.\n    **Solution:**\n\n\n\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    # Historical returns of a portfolio\n    portfolio_returns <- c(-0.05, 0.1, 0.03, -0.02, 0.04)\n    \n    # Confidence level (e.g., 95%)\n    alpha <- 0.05\n    \n    # Calculating VaR\n    VaR <- quantile(portfolio_returns, alpha)\n    \n    # Output the VaR\n    print(VaR)\n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    \n    ```\n        5% \n    -0.044 \n    ```\n    \n    \n    :::\n    :::\n\n\n\n\n\n    **Explanation:** VaR measures the maximum expected loss over a given period under normal market conditions at a specified confidence level (here, 95%).\n\n7.  Bayesian Update in Stock Forecasting:\n    **Objective:** To perform a Bayesian update for a stock price prediction.\n    **Solution:**\n\n\n\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    # Uniform prior distribution\n    prior <- dbeta(1, 2, 1)\n    \n    # Binomial likelihood based on new evidence (e.g., 6 increases in 10 periods)\n    likelihood <- dbinom(6, size=10, prob=0.5)\n    \n    # Calculating the posterior distribution\n    posterior <- prior * likelihood\n    \n    # Output the posterior probability\n    print(posterior)\n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    \n    ```\n    [1] 0.4101562\n    ```\n    \n    \n    :::\n    :::\n\n\n\n\n\n    **Explanation:** Bayesian update combines prior belief (uniform distribution in this case) with new evidence (likelihood) to revise the belief about a stock's price movement.\n\n8.  Hypothesis Testing in Financial Returns:\n    **Objective:** To conduct and interpret a hypothesis test comparing a new investment strategy to market returns.\n    **Solution:**\n\n\n\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    # Returns from a new investment strategy\n    strategy_returns <- c(0.07, 0.08, 0.09, 0.06, 0.1)\n    \n    # Market average returns for comparison\n    market_returns <- c(0.05, 0.05, 0.05, 0.05, 0.05)\n    \n    # Performing a t-test\n    t_test_result <- t.test(strategy_returns, market_returns)\n    \n    # Output the t-test results\n    print(t_test_result)\n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    \n    ```\n    \n    \tWelch Two Sample t-test\n    \n    data:  strategy_returns and market_returns\n    t = 4.2426, df = 4, p-value = 0.01324\n    alternative hypothesis: true difference in means is not equal to 0\n    95 percent confidence interval:\n     0.01036757 0.04963243\n    sample estimates:\n    mean of x mean of y \n         0.08      0.05 \n    ```\n    \n    \n    :::\n    :::\n\n\n\n\n\n    **Explanation:** The t-test assesses whether the mean returns of the new strategy are significantly different from the market average. The p-value indicates the probability of observing such a difference if there were no real difference.\n\n9.  Complex Time Series Analysis:\n    **Objective:** To fit an ARIMA model to a financial time series and forecast future values.\n    **Solution:**\n\n\n\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    # Assuming stock_prices is a time series object\n    # Install and load the forecast package\n    library(forecast)\n    \n    # Fitting an ARIMA model\n    arima_model <- auto.arima(stock_prices)\n    \n    # Forecasting the next value\n    forecast_result <- forecast(arima_model, h=1)\n    \n    # Output the forecast\n    print(forecast_result)\n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    \n    ```\n      Point Forecast    Lo 80    Hi 80    Lo 95    Hi 95\n    6          122.2 119.7349 124.6651 118.4299 125.9701\n    ```\n    \n    \n    :::\n    :::\n\n\n\n\n\n    **Explanation:** `auto.arima()` function automatically selects the best ARIMA model for the time series data. The `forecast()` function then uses this model to predict future values (here, the forecast horizon is 1).\n\n10. Portfolio Optimization:\nThe Markowitz model involves optimizing a portfolio by finding the best combination of assets that minimizes risk (variance) for a given expected return, or maximizes return for a given level of risk. This is achieved by adjusting the weights of each asset in the portfolio.\n\n**Solution:**\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load the quadprog library for quadratic programming\nlibrary(quadprog)\n\n# Generate a sequence of 10 dates, one day apart\nN = 10\nstart_date <- as.Date(\"2023-01-01\")\ndates <- seq.Date(from = start_date, by = \"day\", length.out = N)\n\n# Define the historical returns for four assets\nhistorical_returns <- data.frame(\"Asset1\"=rnorm(N, mean = 0.04),                                  \n                                 \"Asset2\"=rnorm(N, mean = 0.03),                                 \n                                 \"Asset3\"=rnorm(N, mean = 0.09),                                 \n                                 \"Asset4\"=rnorm(N, mean = 0.01))\nrownames(historical_returns) <- dates\n\n# Calculate the covariance matrix of returns\nDmat <- cov(historical_returns)\n\n# Define dvec as zero for minimum variance portfolio\ndvec <- rep(0, ncol(historical_returns))\n\n# Define the constraints (sum of weights = 1)\n# Amat needs to have as many rows as there are assets plus one for the sum constraint\nAmat <- cbind(1, diag(ncol(historical_returns)))\nbvec <- c(1, rep(0, ncol(historical_returns)))\n\n# Specify and solve the optimization problem\nsol <- solve.QP(Dmat, dvec, Amat, bvec, meq = 1)\n\n# Extract the optimal weights\noptimal_weights <- sol$solution\n\n# Print the optimal weights\nprint(optimal_weights)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 3.116054e-01 5.532016e-01 1.351931e-01 8.111840e-18\n```\n\n\n:::\n:::\n\n\n\n\n\n**Interpretation**\n\n1.  **Setting Up the Data**:    \n    -   The code first sets up a simulated historical return data for four assets over ten days. This is necessary as the Markowitz model requires historical return data to calculate asset weights.\n\n2.  **Covariance Matrix**:    \n    -   `Dmat` is calculated as the covariance matrix of the asset returns. It represents the risk relationships between each pair of assets, crucial in determining how asset prices move relative to each other.\n\n3.  **Defining Optimization Parameters**:    \n    -   The vector `dvec` is set to zero since the goal is to minimize variance without targeting a specific return.    \n    -   `Amat` combines an equality constraint (that the sum of the asset weights equals 1) with non-negativity constraints (each asset weight must be zero or positive).\n\n4.  **Quadratic Programming Problem**:    \n    -   The `solve.QP` function is used to solve the quadratic programming problem. It aims to find the asset weights that minimize the overall portfolio variance subject to the given constraints.\n\n5.  **Optimal Weights**:    \n    -   The solution `sol$solution` provides the optimal weights for each asset. These weights represent how much of the portfolio should be allocated to each asset to achieve minimum risk.\n\n6.  **Result**:    \n    -   The output is a set of portfolio weights that minimize the portfolio's variance (risk), considering the historical return covariance of the assets and the constraints (total weight equals 1, non-negative weights). This represents the most risk-efficient allocation of assets in the portfolio under the given conditions.\n\n::: callout-important\nThese solutions provide a mix of conceptual explanations and practical R code, offering a comprehensive understanding of each question's objective and methodology.\n:::\n\n## Chapter 2: Toolkit\n\n**Theoretical Questions Solutions:**\n*Easier:*\n\n1.  **R's Role in Financial Analysis:**    \n    -   Solution: R offers extensive packages for statistical analysis and data handling, making it ideal for analyzing complex financial data. Its powerful graphical capabilities enable clear visualization of financial trends and patterns.\n\n2.  **Advantages of Cloud Computing in Finance:**    \n    -   Solution: Cloud platforms like Posit Cloud provide scalability, easy access to advanced analytics tools, and collaborative features, essential for handling large datasets and complex financial models.\n\n3.  **Data Visualization Importance:**    \n    -   Solution: Data visualization is key in financial analysis for interpreting complex data sets and communicating findings effectively. `ggplot2` offers a versatile, layer-based plotting system, making complex visualizations more intuitive.\n\n4.  **Version Control with Git:**    \n    -   Solution: Version control is crucial for managing changes in code, especially in collaborative projects. Git allows tracking of changes, reverting to previous versions, and effective team collaboration.\n\n5.  **Growth Mindset in Data Science:**    \n    -   Solution: A growth mindset encourages continual learning and adaptability, crucial in a field like financial data analytics, where technologies and market conditions are constantly evolving.\n\n*Advanced:*\n\n6.  **Statistical vs. Machine Learning Approaches:**    \n    -   Solution: Statistical modeling typically involves hypothesis-driven models, while machine learning focuses on prediction using data-driven models. Both approaches are valuable in financial data analysis, each with strengths in different scenarios.\n\n7.  **Reproducibility Challenges:**    \n    -   Solution: Challenges include data accessibility, software environment consistency, and clear documentation. Solutions involve using version control, containerization tools, and comprehensive documentation of analysis steps.\n\n8.  **Collaborative Coding with Git and GitHub:**    \n    -   Solution: Git and GitHub facilitate version control, issue tracking, and code review, supporting a collaborative workflow. This ensures code integrity and effective team collaboration in financial analysis projects.\n\n9.  **Tidyverse Ecosystem:**    \n    -   Solution: The Tidyverse provides a consistent and user-friendly syntax for data import, cleaning, manipulation, and visualization, streamlining the data analysis process and enhancing productivity in financial data analytics.\n\n10. **Modular Coding for Financial Analysis:**    \n    -   Solution: Modular coding in R improves code readability, reusability, and testing. In financial analysis, where models can be complex, this approach enables easier maintenance and collaboration.\n\n**Practical Questions Solutions:**\n*Easier:*\n\n1.  **Basic R Data Manipulation:**\n\n\n\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    data <- data %>% mutate(percent_change = (stock_price - lag(stock_price)) / lag(stock_price) * 100)\n    ```\n    :::\n\n\n\n\n\n    Interpretation: Percentage change helps identify trends in stock prices, indicating potential growth or decline.\n\n2.  **Creating Plots in R:**\n\n\n\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    ggplot(data, aes(x = date, y = stock_price)) + geom_line()\n    ```\n    :::\n\n\n\n\n\n    `Interpretation: Line charts provide a clear view of stock price trends over time, aiding in investment decisions.\n\n3.  **Git Basics:**\n    Commands: `git init`, `git add .`, `git commit -m \"Initial commit\"`\n    Benefits: Ensures code versioning, allows tracking of changes, and facilitates team collaboration.\n\n4.  **Data Cleaning in R:**\n\n\n\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    data <- data %>% mutate(stock_price = ifelse(is.na(stock_price), mean(stock_price, na.rm = TRUE), stock_price))\n    ```\n    :::\n\n\n\n\n\n    Implications: Handling missing data prevents biases and errors in financial analysis.\n\n5.  **Basic Linear Regression in R:**\n\n\n\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    model <- lm(stock_price ~ stock_id, data = data)\n    summary(model)\n    ```\n    :::\n\n\n\n\n\n    Interpretation: The model provides insights into how stock prices are related to their IDs, which could correlate with other financial factors.\n\n*Advanced:*\n\n6.  **Advanced Financial Modeling:**\n\n\n\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    model <- auto.arima(stock_data$stock_price)\n    forecast(model)\n    ```\n    :::\n\n\n\n\n\n    Assumptions: Assumes stock prices follow an ARIMA process. Limitations include potential overfitting and sensitivity to data anomalies.\n\n7.  **Machine Learning Application:**\n\n\n\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    model <- train(stock_price ~ ., data = stock_data, method = \"rf\")\n    ```\n    :::\n\n\n\n\n\n    Choice and Effectiveness: Random Forest is chosen for its ability to handle non-linear relationships in data, useful in complex financial markets.\n\n8.  **Reproducible Analysis with Quarto:**\n    Use Quarto to create a document that combines R code for financial analysis, outputs, and narrative.\n    Importance: Ensures that financial analyses can be reliably reproduced and verified.\n\n9.  **Tidyverse for Complex Data Manipulation:**\n\n\n\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    stock_data %>% group_by(stock_id) %>% summarize(average_price = mean(stock_price))\n    ```\n    :::\n\n\n\n\n\n    Benefit: This manipulation provides insights into the average performance of each stock, crucial for portfolio analysis.\n\n10. **Collaborative Financial Project using GitHub:**\n    Workflow: Clone a repository, create branches for features, use pull requests for merging.\n    Benefits: Enhances collaboration, ensures code review, and maintains project organization.\n\nThese solutions offer a comprehensive understanding of both the theoretical concepts and practical applications in financial data analytics using R and related tools.\n\n## Chapter 4: Time series models\n\nHere are the R coded solutions with detailed explanations for each exercise:\n\nExercise 1:\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Install and load necessary packages\nlibrary(tidyquant)\n\n# Download historical stock prices for Apple Inc. (AAPL)\naapl_data <- tq_get(\"AAPL\", from = \"2018-01-01\", to = \"2023-12-31\")\n\n# Extract the closing prices and calculate daily returns\nclosing_prices <- aapl_data$close\ndaily_returns <- (closing_prices - lag(closing_prices)) / lag(closing_prices)\n\n# Plot the closing prices over time\nplot(closing_prices, main = \"Apple Inc. (AAPL) Closing Prices\",\n     xlab = \"Time\", ylab = \"Price\", col = \"blue\")\n```\n\n::: {.cell-output-display}\n![](solutions_files/figure-html/unnamed-chunk-18-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# Plot the daily returns over time     \nplot(daily_returns, main = \"Apple Inc. (AAPL) Daily Returns\",\n     xlab = \"Time\", ylab = \"Return\", col = \"red\")\n```\n\n::: {.cell-output-display}\n![](solutions_files/figure-html/unnamed-chunk-18-2.png){width=672}\n:::\n:::\n\n\n\n\n\n\nExplanation:\n- The `tidyquant` package is used to download historical stock price data from Yahoo Finance.\nCertainly! Here's the complete explanation for Exercise 1:\n\nExplanation:\n- The `tidyquant` package is used to download historical stock price data from Yahoo Finance.\n- `tq_get()` retrieves the data for the specified ticker symbol (AAPL) and date range.\n- The closing prices are extracted from the downloaded data using `aapl_data$close`.\n- Daily returns are calculated based on the closing prices using the formula `(closing_prices - lag(closing_prices)) / lag(closing_prices)`.\n- The closing prices and daily returns are plotted using the `plot()` function.\n- The plots help identify trends, seasonality, or unusual patterns in the price and return series.\n\nExercise 2:\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(signal)\n\n# Apply smoothing techniques\nsma_20 <- SMA(closing_prices, n = 20)\nema_0.1 <- EMA(closing_prices, n = 20, wilder = FALSE, ratio = 0.1)\nsg_filtered <- sgolayfilt(closing_prices, p = 3, n = 21)\nlowess_smoothed <- lowess(closing_prices)\n\n# Plot the original price series and smoothed series\nplot(closing_prices, main = \"Apple Inc. (AAPL) Closing Prices with Smoothing\",\n     xlab = \"Time\", ylab = \"Price\", col = \"black\")\nlines(sma_20, col = \"blue\")\nlines(ema_0.1, col = \"red\")\nlines(sg_filtered, col = \"green\")\nlines(lowess_smoothed$y, col = \"purple\")\nlegend(\"topleft\", legend = c(\"Original\", \"SMA (20)\", \"EMA (0.1)\", \"Savitzky-Golay\", \"Lowess\"),\n       col = c(\"black\", \"blue\", \"red\", \"green\", \"purple\"), lty = 1)\n```\n\n::: {.cell-output-display}\n![](solutions_files/figure-html/unnamed-chunk-19-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\nExplanation:\n- The `SMA()` function calculates the Simple Moving Average with a window size of 20 days.\n- The `EMA()` function calculates the Exponential Moving Average with a smoothing factor of 0.1.\n- The `sgolayfilt()` function applies the Savitzky-Golay filter with a polynomial order of 3 and a window size of 21.\n- The `lowess()` function applies Lowess smoothing with default parameters.\n- The original price series and smoothed series are plotted on the same graph using `plot()` and `lines()` functions.\n- The differences between the smoothing methods and their effectiveness in capturing the underlying trend can be observed from the plot.\n\nExercise 3:\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tseries)\nlibrary(tidyverse)\n\n# Test for stationarity using the ADF test\nadf_test <- adf.test(closing_prices)\nprint(adf_test)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tAugmented Dickey-Fuller Test\n\ndata:  closing_prices\nDickey-Fuller = -2.7663, Lag order = 11, p-value = 0.2539\nalternative hypothesis: stationary\n```\n\n\n:::\n\n```{.r .cell-code}\n# If non-stationary, apply differencing\nif (adf_test$p.value > 0.05) {\n  differenced_prices <- diff(closing_prices)\n  # remove NAs from differenced series\n  differenced_prices <- na.omit(differenced_prices)\n  adf_test_diff <- adf.test(differenced_prices)\n  print(adf_test_diff)\n}\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tAugmented Dickey-Fuller Test\n\ndata:  differenced_prices\nDickey-Fuller = -11.139, Lag order = 11, p-value = 0.01\nalternative hypothesis: stationary\n```\n\n\n:::\n:::\n\n\n\n\n\n\nExplanation:\n- The `adf.test()` function performs the Augmented Dickey-Fuller (ADF) test to check for stationarity.\n- If the p-value of the ADF test is greater than 0.05, the series is considered non-stationary.\n- In case of non-stationarity, differencing is applied using the `diff()` function.\n- The ADF test is performed again on the differenced series to confirm stationarity.\n\nExercise 4:\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Fit an ARIMA model\narima_model <- auto.arima(closing_prices)\nprint(summary(arima_model))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSeries: closing_prices \nARIMA(0,1,1) with drift \n\nCoefficients:\n          ma1   drift\n      -0.0418  0.0991\ns.e.   0.0264  0.0545\n\nsigma^2 = 4.891:  log likelihood = -3335.72\nAIC=6677.45   AICc=6677.47   BIC=6693.41\n\nTraining set error measures:\n                       ME     RMSE      MAE         MPE     MAPE      MASE\nTraining set 5.192884e-05 2.209461 1.510575 -0.04057672 1.404191 0.9961854\n                    ACF1\nTraining set 0.001037842\n```\n\n\n:::\n\n```{.r .cell-code}\n# Forecast the next 30 days\nforecast_30 <- forecast(arima_model, h = 30)\nplot(forecast_30, main = \"ARIMA Forecast for Apple Inc. (AAPL) Closing Prices\",\n     xlab = \"Time\", ylab = \"Price\")\n```\n\n::: {.cell-output-display}\n![](solutions_files/figure-html/unnamed-chunk-21-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\nExplanation:\n- The `auto.arima()` function automatically selects the best ARIMA model parameters based on the AIC criterion.\n- The `summary()` function provides a summary of the fitted ARIMA model, including the coefficients and their significance.\n- The `forecast()` function is used to forecast the next 30 days of stock prices based on the fitted ARIMA model.\n- The forecasted values are plotted along with the original price series using the `plot()` function.\n\nExercise 5:\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Install and load necessary packages\nlibrary(rugarch)\n\n# Specify the GARCH(1,1) model\nspec <- ugarchspec(mean.model = list(armaOrder = c(0, 0)),\n                   variance.model = list(garchOrder = c(1, 1),\n                                         model = \"sGARCH\",\n                                        submodel = NULL),\n                   distribution.model = \"norm\",\n                   fixed.pars = list(mu = 0, omega = 0.1, alpha1 = 0.1, beta1 = 0.8))\n\n# Simulate a GARCH(1,1) process\ngarch_sim <- ugarchpath(spec, n.sim = 1000, m.sim = 1)\n\n# Fit a GARCH(1,1) model to the simulated data\ngarch_fit <- ugarchfit(spec, garch_sim@path$seriesSim)\nprint(garch_fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n*------------------------------------*\n*          GARCH Model Filter        *\n*------------------------------------*\n\nConditional Variance Dynamics \t\n--------------------------------------\nGARCH Model\t: sGARCH(1,1)\nMean Model\t: ARFIMA(0,0,0)\nDistribution\t: norm \n\nFilter Parameters\n---------------------------------------\n          \nmu     0.0\nomega  0.1\nalpha1 0.1\nbeta1  0.8\n\nLogLikelihood : -1423.172 \n\nInformation Criteria\n---------------------------------------\n                   \nAkaike       2.8463\nBayes        2.8463\nShibata      2.8463\nHannan-Quinn 2.8463\n\nWeighted Ljung-Box Test on Standardized Residuals\n---------------------------------------\n                        statistic p-value\nLag[1]                    0.01061  0.9180\nLag[2*(p+q)+(p+q)-1][2]   0.01734  0.9830\nLag[4*(p+q)+(p+q)-1][5]   1.77145  0.6733\nd.o.f=0\nH0 : No serial correlation\n\nWeighted Ljung-Box Test on Standardized Squared Residuals\n---------------------------------------\n                        statistic p-value\nLag[1]                     0.1844  0.6677\nLag[2*(p+q)+(p+q)-1][5]    0.7663  0.9099\nLag[4*(p+q)+(p+q)-1][9]    1.6719  0.9403\nd.o.f=2\n\nWeighted ARCH LM Tests\n---------------------------------------\n            Statistic Shape Scale P-Value\nARCH Lag[3]   0.00827 0.500 2.000  0.9275\nARCH Lag[5]   0.70795 1.440 1.667  0.8210\nARCH Lag[7]   1.45969 2.315 1.543  0.8294\n\n\nSign Bias Test\n---------------------------------------\n                   t-value   prob sig\nSign Bias           0.9717 0.3314    \nNegative Sign Bias  0.4637 0.6429    \nPositive Sign Bias  0.2760 0.7826    \nJoint Effect        5.1283 0.1626    \n\n\nAdjusted Pearson Goodness-of-Fit Test:\n---------------------------------------\n  group statistic p-value(g-1)\n1    20     17.68       0.5439\n2    30     25.70       0.6415\n3    40     45.12       0.2314\n4    50     45.90       0.5996\n```\n\n\n:::\n\n```{.r .cell-code}\n# Compare the estimated parameters with the true values\nprint(coef(garch_fit))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n    mu  omega alpha1  beta1 \n   0.0    0.1    0.1    0.8 \n```\n\n\n:::\n\n```{.r .cell-code}\nprint(garch_sim@model$pars)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n         Level Fixed Include Estimate LB UB\nmu         0.0     1       1        0 NA NA\nar         0.0     0       0        0 NA NA\nma         0.0     0       0        0 NA NA\narfima     0.0     0       0        0 NA NA\narchm      0.0     0       0        0 NA NA\nmxreg      0.0     0       0        0 NA NA\nomega      0.1     1       1        0 NA NA\nalpha1     0.1     1       1        0 NA NA\nbeta1      0.8     1       1        0 NA NA\ngamma      0.0     0       0        0 NA NA\neta1       0.0     0       0        0 NA NA\neta2       0.0     0       0        0 NA NA\ndelta      0.0     0       0        0 NA NA\nlambda     0.0     0       0        0 NA NA\nvxreg      0.0     0       0        0 NA NA\nskew       0.0     0       0        0 NA NA\nshape      0.0     0       0        0 NA NA\nghlambda   0.0     0       0        0 NA NA\nxi         0.0     0       0        0 NA NA\n```\n\n\n:::\n:::\n\n\n\n\n\n\nIn the updated code, the model specification is defined using `ugarchspec()` with the following changes:\n- The `fixed.pars` argument is used to specify the parameter values directly, ensuring that the names match the expected names.\n- The `submodel` argument in the `variance.model` is set to `NULL` to use the default GARCH(1,1) parameterization.\n\nBy specifying the parameter values using `fixed.pars` with the correct names, the error should be resolved, and the code should run without issues.\n\nHere's the explanation of the changes made:\n- `mu`: Represents the mean of the GARCH process. It is set to 0 in this example.\n- `omega`: Represents the constant term in the GARCH variance equation. It is set to 0.1 in this example.\n- `alpha1`: Represents the coefficient of the squared residual term in the GARCH variance equation. It is set to 0.1 in this example.\n- `beta1`: Represents the coefficient of the lagged variance term in the GARCH variance equation. It is set to 0.8 in this example.\n\nBy providing the parameter values directly using `fixed.pars`, you ensure that the parameter names match the expected names, and the `ugarchpath()` function should run without the parameter mismatch error.\n\nRemember to adjust the parameter values according to your specific requirements and the characteristics of your GARCH process.\n\nExercise 6:\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Download historical stock prices for two related companies\nlibrary(tidyquant)\naapl_data <- tq_get(\"AAPL\", from = \"2018-01-01\", to = \"2023-12-31\")\nmsft_data <- tq_get(\"MSFT\", from = \"2018-01-01\", to = \"2023-12-31\")\n\n# Extract the closing prices\naapl_prices <- aapl_data$close\nmsft_prices <- msft_data$close\n\n# Perform cointegration analysis\nlibrary(urca)\ncoint_test <- ca.jo(cbind(aapl_prices, msft_prices), type = \"trace\", K = 2, ecdet = \"none\", spec = \"transitory\")\nsummary(coint_test)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n###################### \n# Johansen-Procedure # \n###################### \n\nTest type: trace statistic , with linear trend \n\nEigenvalues (lambda):\n[1] 0.0096484803 0.0002033327\n\nValues of teststatistic and critical values of test:\n\n          test 10pct  5pct  1pct\nr <= 1 |  0.31  6.50  8.18 11.65\nr = 0  | 14.92 15.66 17.95 23.52\n\nEigenvectors, normalised to first column:\n(These are the cointegration relations)\n\n               aapl_prices.l1 msft_prices.l1\naapl_prices.l1      1.0000000      1.0000000\nmsft_prices.l1     -0.6176325      0.3785062\n\nWeights W:\n(This is the loading matrix)\n\n              aapl_prices.l1 msft_prices.l1\naapl_prices.d   -0.017308401  -0.0001983142\nmsft_prices.d   -0.009527852  -0.0006805164\n```\n\n\n:::\n\n```{.r .cell-code}\n# Estimate a VECM if cointegration is found\nif (coint_test@teststat[1] > coint_test@cval[1,2]) {\n  vecm_model <- cajorls(coint_test, r = 1)\n  print(summary(vecm_model$rlm))\n}\n```\n:::\n\n\n\n\n\n\nExplanation:\n- Historical stock prices for two related companies (AAPL and MSFT) are downloaded using `tq_get()` from the `tidyquant` package.\n- The closing prices for both stocks are extracted using `aapl_data$close` and `msft_data$close`.\n- The `urca` package is used for cointegration analysis.\n- `ca.jo()` performs the Johansen cointegration test to determine if there is a long-run relationship between the two price series.\n- If cointegration is found (i.e., the test statistic is greater than the critical value), a Vector Error Correction Model (VECM) is estimated using `cajorls()`.\n- The summary of the VECM is printed for interpretation.\n\nExercise 7:\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Conduct Granger causality test\nlibrary(lmtest)\ngrangertest(aapl_prices ~ msft_prices, order = 1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nGranger causality test\n\nModel 1: aapl_prices ~ Lags(aapl_prices, 1:1) + Lags(msft_prices, 1:1)\nModel 2: aapl_prices ~ Lags(aapl_prices, 1:1)\n  Res.Df Df      F  Pr(>F)   \n1   1505                     \n2   1506 -1 10.054 0.00155 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n\n```{.r .cell-code}\ngrangertest(msft_prices ~ aapl_prices, order = 1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nGranger causality test\n\nModel 1: msft_prices ~ Lags(msft_prices, 1:1) + Lags(aapl_prices, 1:1)\nModel 2: msft_prices ~ Lags(msft_prices, 1:1)\n  Res.Df Df      F Pr(>F)\n1   1505                 \n2   1506 -1 0.4591 0.4982\n```\n\n\n:::\n:::\n\n\n\n\n\n\nExplanation:\n- The `lmtest` package is used for conducting the Granger causality test.\n- `grangertest()` performs the Granger causality test to determine if the price of one stock can be used to predict the price of the other stock.\n- The test is conducted in both directions: AAPL prices predicting MSFT prices and vice versa.\n- The test results provide insights into the lead-lag relationship between the two stocks and their implications for investment and risk management strategies.\n\nExercise 8:\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Simulate a random walk process\nset.seed(123)\nn_steps <- length(closing_prices)\ndrift <- 0.001\nvolatility <- 0.02\ninitial_price <- 100\n\nsteps <- sample(c(-1, 1), size = n_steps, replace = TRUE)\nsteps[1] <- 0\nreturns <- drift + volatility * steps\nsim_prices <- initial_price * exp(cumsum(returns))\n\n# Plot the simulated price series\nplot(sim_prices, type = \"l\", main = \"Simulated Random Walk Process\",\n     xlab = \"Time\", ylab = \"Price\")\n```\n\n::: {.cell-output-display}\n![](solutions_files/figure-html/unnamed-chunk-25-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# Calculate daily returns\nsim_returns <- diff(log(sim_prices))\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\n\n# Compare with real stock price data\nprice_data <- data.frame(Time = 1:length(closing_prices[1:length(sim_prices)]),\n                         Real = as.numeric(closing_prices[1:length(sim_prices)]),\n                         Simulated = as.numeric(sim_prices))\n\nggplot(data = price_data, aes(x = Time)) +\n  geom_line(aes(y = Real, color = \"Real\"), linewidth = 1) +\n  geom_line(aes(y = Simulated, color = \"Simulated\"), linewidth = 1) +\n  scale_color_manual(values = c(\"Real\" = \"blue\", \"Simulated\" = \"red\")) +\n  labs(title = \"Comparison of Real and Simulated Price Series\",\n       x = \"Time\",\n       y = \"Price\") +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](solutions_files/figure-html/unnamed-chunk-26-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# Compare return distributions\nreturn_data <- data.frame(Return = c(daily_returns, sim_returns),\n                          Type = c(rep(\"Real\", length(daily_returns)),\n                                   rep(\"Simulated\", length(sim_returns))))\n\nggplot(data = return_data, aes(x = Return, fill = Type)) +\n  geom_histogram(aes(y = ..density..), alpha = 0.7, position = \"identity\", bins = 30) +\n  geom_density(alpha = 0.5) +\n  scale_fill_manual(values = c(\"Real\" = \"blue\", \"Simulated\" = \"red\")) +\n  labs(title = \"Comparison of Return Distributions\",\n       x = \"Return\",\n       y = \"Density\") +\n  facet_wrap(~ Type, ncol = 2) +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](solutions_files/figure-html/unnamed-chunk-26-2.png){width=672}\n:::\n:::\n\n\n\n\n\n\nExplanation:\n1. Price Series Comparison:\n   - We create a data frame `price_data` that contains the time index, real prices, and simulated prices.\n   - Using `ggplot()`, we create a line plot with the time index on the x-axis and the price values on the y-axis.\n   - We add two `geom_line()` layers to plot the real prices (in blue) and simulated prices (in red).\n   - We set the colors manually using `scale_color_manual()` and provide appropriate labels using `labs()`.\n   - Finally, we apply the `theme_minimal()` theme for a cleaner plot appearance.\n\n2. Return Distributions Comparison:\n   - We create a data frame `return_data` that combines the real returns and simulated returns, along with a type indicator.\n   - Using `ggplot()`, we create a histogram plot with the return values on the x-axis and the density on the y-axis.\n   - We use `geom_histogram()` to plot the histograms, with `..density..` mapping to show the density instead of count.\n   - We set `alpha` to 0.7 for transparency and `position = \"identity\"` to overlay the histograms.\n   - We add a `geom_density()` layer to display the density curves on top of the histograms.\n   - We set the fill colors manually using `scale_fill_manual()`.\n   - We use `facet_wrap()` to create separate panels for the real and simulated return distributions.\n   - Finally, we apply the `theme_minimal()` theme for a cleaner plot appearance.\n\nThe resulting plots will show the comparison of the real and simulated price series over time, as well as the comparison of their return distributions side by side.\n\nExplanation:\n- A random walk process is simulated with specified drift and volatility parameters.\n- The simulated price series is plotted using `plot()` with a line type.\n- Daily returns are calculated from the simulated price series using `diff()` and `log()`.\n- The simulated price series is compared with the real stock price data by plotting them together using `ggplot()` and `geom_line()`.\n- The return distributions of the real and simulated data are compared using histograms and density plots with `ggplot()`, `geom_histogram()`, and `geom_density()`.\n- The similarities and differences between the real and simulated series, as well as their implications for financial modeling and forecasting, can be discussed based on the visual comparison and return distributions.\n\n\n\nExercise 9: Backtesting ARMA Models:\n\n```R\nlibrary(quantmod)\nlibrary(forecast)\n\n# Download historical stock prices\ngetSymbols(\"AAPL\", from = \"2016-01-01\", to = \"2021-12-31\")\nprices <- Cl(AAPL)\n\n# Split the data into training and testing sets\ntrain_data <- prices[\"2016-01-01/2019-12-31\"]\ntest_data <- prices[\"2020-01-01/2021-12-31\"]\n\n# Fit an ARMA(1, 1) model on the training set\nmodel <- arima(train_data, order = c(1, 0, 1))\n\n# Make predictions for the testing set\npredictions <- forecast(model, h = length(test_data))\n\n# Calculate MAE and RMSE\nmae <- mean(abs(test_data - predictions$mean))\nrmse <- sqrt(mean((test_data - predictions$mean)^2))\n\n# Rolling window approach\nwindow_size <- 252  # One year of trading days\nmae_rolling <- c()\nrmse_rolling <- c()\n\nfor (i in seq_along(test_data)) {\n  if (i < window_size) next\n  \n  train_data_rolling <- prices[(i - window_size + 1):(i - 1)]\n  test_data_rolling <- prices[i]\n  \n  model_rolling <- arima(train_data_rolling, order = c(1, 0, 1))\n  prediction_rolling <- forecast(model_rolling, h = 1)\n  \n  mae_rolling <- c(mae_rolling, abs(test_data_rolling - prediction_rolling$mean[1]))\n  rmse_rolling <- c(rmse_rolling, (test_data_rolling - prediction_rolling$mean[1])^2)\n}\n\n# Compare static and rolling window approaches\ncat(\"Static Approach:\\n\")\ncat(\"MAE:\", mae, \"\\n\")\ncat(\"RMSE:\", rmse, \"\\n\\n\")\n\ncat(\"Rolling Window Approach:\\n\")\ncat(\"MAE:\", mean(mae_rolling), \"\\n\")\ncat(\"RMSE:\", sqrt(mean(rmse_rolling)), \"\\n\")\n```\n\nExcercise 10: Cross-Validating GARCH Models:\n\n```R\nlibrary(quantmod)\nlibrary(rugarch)\n\n# Download historical stock prices\ngetSymbols(\"AAPL\", from = \"2016-01-01\", to = \"2021-12-31\")\nreturns <- diff(log(Cl(AAPL)))\n\n# Define the time series cross-validation function\nts_cv <- function(data, n_splits, model_func) {\n  n <- length(data)\n  split_size <- floor(n / n_splits)\n  \n  mse_scores <- c()\n  mae_scores <- c()\n  \n  for (i in 1:(n_splits - 1)) {\n    train_start <- (i - 1) * split_size + 1\n    train_end <- i * split_size\n    test_start <- train_end + 1\n    test_end <- min(test_start + split_size - 1, n)\n    \n    train_data <- data[train_start:train_end]\n    test_data <- data[test_start:test_end]\n    \n    model <- model_func(train_data)\n    predictions <- predict(model, n.ahead = length(test_data), cond.dist = \"norm\")\n    \n    mse <- mean((test_data - predictions$mean)^2)\n    mae <- mean(abs(test_data - predictions$mean))\n    \n    mse_scores <- c(mse_scores, mse)\n    mae_scores <- c(mae_scores, mae)\n  }\n  \n  return(list(mse = mean(mse_scores), mae = mean(mae_scores)))\n}\n\n# Define the GARCH model function\ngarch_model <- function(data) {\n  spec <- ugarchspec(variance.model = list(model = \"sGARCH\", garchOrder = c(1, 1)),\n                     mean.model = list(armaOrder = c(0, 0)))\n  model <- ugarchfit(spec, data)\n  return(model)\n}\n\n# Perform time series cross-validation\ncv_results <- ts_cv(returns, n_splits = 5, model_func = garch_model)\ncat(\"Cross-Validation Results:\\n\")\ncat(\"MSE:\", cv_results$mse, \"\\n\")\ncat(\"MAE:\", cv_results$mae, \"\\n\\n\")\n\n# Fit GARCH model on the entire dataset\nfull_model <- garch_model(returns)\ncat(\"Full Model Results:\\n\")\ncat(\"MSE:\", mean(full_model@fit$residuals^2), \"\\n\")\ncat(\"MAE:\", mean(abs(full_model@fit$residuals)), \"\\n\")\n```\n\nExcercise 11: Comparing Forecast Models\n\n```R\nlibrary(quantmod)\nlibrary(forecast)\nlibrary(prophet)\nlibrary(Metrics)\n\n# Download historical sales data\nsales_data <- getSymbols(\"OLP\", from = \"2011-01-01\", to = \"2021-12-31\", auto.assign = FALSE)\nmonthly_sales <- to.monthly(sales_data)[,4]\n\n# Function to perform rolling window cross-validation\nrolling_cv <- function(data, n_splits, model_func) {\n  n <- length(data)\n  split_size <- floor(n / n_splits)\n  \n  mae_scores <- c()\n  rmse_scores <- c()\n  mape_scores <- c()\n  \n  for (i in 1:(n_splits - 1)) {\n    train_start <- (i - 1) * split_size + 1\n    train_end <- i * split_size\n    test_start <- train_end + 1\n    test_end <- min(test_start + split_size - 1, n)\n    \n    train_data <- data[train_start:train_end]\n    test_data <- data[test_start:test_end]\n    \n    model <- model_func(train_data)\n    predictions <- forecast(model, h = length(test_data))\n    \n    mae <- mae(test_data, predictions$mean)\n    rmse <- rmse(test_data, predictions$mean)\n    mape <- mape(test_data, predictions$mean)\n    \n    mae_scores <- c(mae_scores, mae)\n    rmse_scores <- c(rmse_scores, rmse)\n    mape_scores <- c(mape_scores, mape)\n  }\n  \n  return(list(mae = mean(mae_scores), rmse = mean(rmse_scores), mape = mean(mape_scores)))\n}\n\n# ARIMA model function\narima_model <- function(data) {\n  model <- auto.arima(data)\n  return(model)\n}\n\n# ETS model function\nets_model <- function(data) {\n  model <- ets(data)\n  return(model)\n}\n\n# Prophet model function\nprophet_model <- function(data) {\n  df <- data.frame(ds = as.Date(time(data)), y = as.numeric(data))\n  model <- prophet(df)\n  return(model)\n}\n\n# Perform rolling window cross-validation for each model\narima_results <- rolling_cv(monthly_sales, n_splits = 5, model_func = arima_model)\nets_results <- rolling_cv(monthly_sales, n_splits = 5, model_func = ets_model)\nprophet_results <- rolling_cv(monthly_sales, n_splits = 5, model_func = prophet_model)\n\n# Compare model performance\ncat(\"ARIMA Results:\\n\")\nprint(arima_results)\ncat(\"\\nETS Results:\\n\")\nprint(ets_results)\ncat(\"\\nProphet Results:\\n\")\nprint(prophet_results)\n\n# Perform Diebold-Mariano test\narima_errors <- monthly_sales - forecast(arima_model(monthly_sales), h = length(monthly_sales))$mean\nets_errors <- monthly_sales - forecast(ets_model(monthly_sales), h = length(monthly_sales))$mean\nprophet_errors <- monthly_sales - forecast(prophet_model(monthly_sales), h = length(monthly_sales))$yhat\n\ndm_test_arima_ets <- dm.test(arima_errors, ets_errors)\ndm_test_arima_prophet <- dm.test(arima_errors, prophet_errors)\ndm_test_ets_prophet <- dm.test(ets_errors, prophet_errors)\n\ncat(\"\\nDiebold-Mariano Test Results:\\n\")\ncat(\"ARIMA vs ETS: p-value =\", dm_test_arima_ets$p.value, \"\\n\")\ncat(\"ARIMA vs Prophet: p-value =\", dm_test_arima_prophet$p.value, \"\\n\")\ncat(\"ETS vs Prophet: p-value =\", dm_test_ets_prophet$p.value, \"\\n\")\n```\n\nExcercise 11: Tuning Hyperparameters with Cross-Validation\n\n```R\nlibrary(quantmod)\nlibrary(forecast)\n\n# Download historical sales data\nsales_data <- getSymbols(\"OLP\", from = \"2011-01-01\", to = \"2021-12-31\", auto.assign = FALSE)\nmonthly_sales <- to.monthly(sales_data)[,4]\n\n# Function to perform time series cross-validation\nts_cv <- function(data, n_splits, model_func, ...) {\n  n <- length(data)\n  split_size <- floor(n / n_splits)\n  \n  mse_scores <- c()\n  \n  for (i in 1:(n_splits - 1)) {\n    train_start <- (i - 1) * split_size + 1\n    train_end <- i * split_size\n    test_start <- train_end + 1\n    test_end <- min(test_start + split_size - 1, n)\n    \n    train_data <- data[train_start:train_end]\n    test_data <- data[test_start:test_end]\n    \n    model <- model_func(train_data, ...)\n    predictions <- forecast(model, h = length(test_data))\n    \n    mse <- mean((test_data - predictions$mean)^2)\n    mse_scores <- c(mse_scores, mse)\n  }\n  \n  return(mean(mse_scores))\n}\n\n# Grid search function\ngrid_search <- function(data, p_values, d_values, q_values) {\n  best_score <- Inf\n  best_params <- NULL\n  \n  for (p in p_values) {\n    for (d in d_values) {\n      for (q in q_values) {\n        score <- ts_cv(data, n_splits = 5, model_func = arima, order = c(p, d, q))\n        \n        if (score < best_score) {\n          best_score <- score\n          best_params <- c(p, d, q)\n        }\n      }\n    }\n  }\n  \n  return(list(best_params = best_params, best_score = best_score))\n}\n\n# Define the range of hyperparameters\np_values <- 0:3\nd_values <- 0:1\nq_values <- 0:3\n\n# Perform grid search\ngrid_search_results <- grid_search(monthly_sales, p_values, d_values, q_values)\ncat(\"Best Hyperparameters:\", grid_search_results$best_params, \"\\n\")\ncat(\"Best Cross-Validation Score:\", grid_search_results$best_score, \"\\n\")\n\n# Fit ARIMA model with the optimal hyperparameters\nbest_model <- arima(monthly_sales, order = grid_search_results$best_params)\n\n# Make predictions for the next 12 months\npredictions <- forecast(best_model, h = 12)\nprint(predictions)\n```\n\nThese sample solutions provide detailed implementations of the practical exercises using R and relevant packages. They cover various aspects of model evaluation and selection, including backtesting, cross-validation, model comparison, and hyperparameter tuning.\n\nThe first exercise demonstrates backtesting an ARMA model on stock price data, comparing static and rolling window approaches. The second exercise shows how to perform time series cross-validation for a GARCH model. The third exercise compares the performance of different forecasting models (ARIMA, ETS, and Prophet) using rolling window cross-validation and the\n",
    "supporting": [
      "solutions_files/figure-html"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}